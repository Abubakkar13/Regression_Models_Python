{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUPERVISED LEARNING\n",
    " \n",
    "Try to learn to predict target (Y) given input (X) \n",
    " \n",
    "2 MAIN FUNCTIONS   \n",
    "- train(X,Y)   \n",
    "- predict(X) \n",
    "\n",
    "TWO TYPES    \n",
    "Classification  \n",
    "    - Labels are discrete  \n",
    "         - Whether or not it will rain tomorrow  \n",
    "         - Whether or not Google’s stock price will rise or fall tomorrow  \n",
    "     \n",
    "Regression  \n",
    "    - Labels are real numbers  \n",
    "        -  Trying to predict mm of rainfall tomorrow  \n",
    "        -  Trying to predict the value of Google’s stock price  \n",
    "\n",
    "UNSUPERVISED LEARNING \n",
    "    - Given X only  \n",
    "    - Try to learn structure of the data, density estimation(p(X)), clustering  \n",
    " \n",
    "DATA TYPES AND SHAPES  \n",
    "    - X = matrix of shape NxD  \n",
    "        - N = number of samples  \n",
    "        - D = number of features  \n",
    "    - Y =  vector of shape N X 1  \n",
    "        - Regression: will contain float values  \n",
    "        - Classification: will contain integers from 0…K-1 where K = # classes  \n",
    "\n",
    "GENERALIZATION \n",
    "    - Generalization: predict accurately not only for data we trained on but new data we haven’t seen before  \n",
    "    - Usually split data into train/test sets to get an idea of how well a mode will generalize  \n",
    "\n",
    "K-NEAREST NEIGHBORS CONCEPTS  \n",
    "    - Basic Premise: To make a prediction, use closest known data points  \n",
    "    - When can KNN fail?  \n",
    "\n",
    "NAÏVE BAYES  \n",
    "    - Grounded in probability, which can be powerful  \n",
    "    - We will purposely make this model not powerful (‘naïve’)  \n",
    "     - Example  \n",
    "        - We want to determine if an email is spam  \n",
    "        - Can look at words like: free, pills, money  \n",
    "        - We want to find  \n",
    "            - P(‘money’|spam)  \n",
    "            - P(‘money’|not spam)  \n",
    "    - What makes this Naïve?  \n",
    "      - Consider p(‘cash’|spam)  \n",
    "      - Is it correlated with p(‘money’|spam)?  \n",
    "      - Probably but we assume independence  \n",
    "      - With naïve Bayes we assume all of the features are independent  \n",
    "      - P(all words | spam) = p(word1 |spam) * p(word2|spam)  \n",
    "\n",
    "NON-NAÏVE BAYES  \n",
    "    - Usually we just call it ‘Bayes Classifier’  \n",
    "    - More generally we can have a ‘Bayes Model’  \n",
    "        - Can either do classification or regression  \n",
    "\n",
    "DESCRIMINATIVE VS GENERATIVE CLASSIFIERS  \n",
    "Discriminative  \n",
    "        - Classifiers like logistic regression model this directly (discriminative)  \n",
    "        - We start with X, we get Y  \n",
    "\n",
    "Generative  \n",
    "    - We start with Y(the class) and model X  \n",
    "    - Think of each class as a ‘data-making machine’  \n",
    "    - It ‘generates’ the data  \n",
    "    - Naïve Bayes  \n",
    "\n",
    "DECISION TREE  \n",
    "    - Basically a bunch of nested if statements  \n",
    "    - What makes it ML is how we choose the conditions  \n",
    "      - Based on information theory  \n",
    "      - One key feature: We only look at one attribute at a time  \n",
    "        - - ch condition checks only 1 column of X  \n",
    "    - Usual-  call these ‘input features’ but called ‘attributes’ when talking about decision trees  \n",
    "    - leaf N- e = where you make the prediction  \n",
    "\n",
    "INFORMATIOENTROPY  \n",
    "    - Related -  variance  \n",
    "    - Wide varianc-  we don’t know much about the data we will get  \n",
    "    - Slim varia- e: we can be more confident about the data we will get  \n",
    "    - Entropy is a measure o much information we get from finding o the value of the random variable  \n",
    "\n",
    "HYPERPARAMETERS  \n",
    "    - Key point: we want the classifier to perform well on data it hasn’t seen before (generalization)  \n",
    "        - Sometimes we call this data ‘test data’  \n",
    "    - If we are using it to choose hyper parameters or a model we might call it ‘validation data’  \n",
    "\n",
    "K-FOLD CROSS VALIDATION\n",
    "    - Popular method for choosing hyper parameters  \n",
    "    - Split data into K parts (typical values for K = 5, 8 ,10)  \n",
    "    - Loop K times  \n",
    "    - In each iteration, take 1 part out (use it for validation), use the rest for training  \n",
    "    - Returns K different scores (accuracies)  \n",
    "    - Can simply use the mean  \n",
    "    - Also use statistical testing to check if one 1 parameter setting is ‘statistically significantly’ better than the other\n",
    "    \n",
    "FEATURE EXTRACTION AND FEATURE SELECTION  \n",
    "Feature Extraction  \n",
    "        - Lesson: we require intimate knowledge of the data  \n",
    "        - Many times, we need ‘domain knowledge’  \n",
    "\n",
    "Feature Selection  \n",
    "    - Suppose you are  a domain expert, and you’ve extracted features  \n",
    "    - How to choose the best ones?  \n",
    "        - Keep too many > over fit  \n",
    "        - We want to keep just the most powerful and discriminatory  \n",
    "\n",
    "Greedy Method  \n",
    "     - Build a classifier for each individual feature, pick the best one via cross-validation  \n",
    "     - Build another set of classifier, all of which contain the first (best) feature, and one other feature. \n",
    "     - Pick the best via cross-validation. Now you have two features  \n",
    "     - Repeat.  \n",
    "\n",
    "Automatic Extraction/Selection  \n",
    "    - Principal component Analysis  \n",
    "    - A dimensionality reduction technique  \n",
    "    - Automatic – doesn’t require domain knowledge  \n",
    "    - Several desirable properties  \n",
    "        - All outputs are uncorrelated (no redundancy)  \n",
    "        - Outputs are sorted by information contained (measure by variance)  \n",
    "        - We choose enough features such that we retain 95% or 99% of the original variance (this is feature selection)  \n",
    "        -  Disadvantage: it’s only a linear transformation  \n",
    "\n",
    "FIT > PREDICT > SCORE  \n",
    "    - Fit aka Train  \n",
    "    - Two functions:  \n",
    "        - Fit(X,Y)  \n",
    "        - Predict(X)  \n",
    "\n",
    "FOR SUPERVISED AND UNSUPERVISED LEARNING \n",
    "    - we know that there’s a cost function to minimize  \n",
    "    - Squared Error (typical for regression)  \n",
    "\n",
    "BOILER PLATE CODE  \n",
    "    - Class MyModel:  \n",
    "    - def predict(sself, X):  \n",
    "    - pass  \n",
    "    - def fit(self, X, Y, eta, T):  \n",
    "    - pass  \n",
    "    - X, Y = load_my_dataset()  \n",
    "    - Model = MyModel()  \n",
    "    - Model.fit(X,Y)  \n",
    "\n",
    "HOW TO CODE INDEPEDENTLY  \n",
    "    - All data is the same – no matter the domain or industry  \n",
    "    - Interfaces to the algorithms are also conveniently all the same: fit/predict  \n",
    "    - Or fit/(transform) in the unsupervised case  \n",
    "    - Mix and Match: Any data, any algorithm  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
